---
title: AI Model Validation Group
description: "The Model Validation group is focused on supporting GitLab teams to make data-driven feature development decisions leveraging ML/AI."
aliases: /handbook/engineering/development/data-science/model-validation
---

## Mission

The AI Model Validation team mission is to support and improve the integrity, reliability, and effectiveness of Generative AI solutions thorough evaluation, validation, and research science processes. We offer a centralized evaluation framework that promotes data-driven decision-making and pragmatic refinement of AI features.

## Direction

[Group direction](https://about.gitlab.com/handbook/engineering/development/data-science/ai-powered/model-validation/)

## Team members

The following people are permanent members of the Model Validation Group:

| Who | Role |
| --- | --- |
| [Hongtao Yang](/handbook/company/team/#hongtaoyang) | ML Engineer |
| [Andras Herczeg](/handbook/company/team/#andrasherczeg) | Senior Backend Engineer |
| [Stephan Rayner](/handbook/company/team/#srayner) | Senior ML Engineer |
| [Tan Le](/handbook/company/team/#tle_gitlab) | Senior ML Engineer |
| [Monmayuri Ray](/handbook/company/team/#mon) | Engineering Manager |
| [Susie Bitters](/handbook/company/team/#susie.bee) | Senior Product Manager |

## How to contact us

- Tag a team member in a merge request or issue
- Post a message in the #g_ai_model_validation Slack channel (internal only)

## Customer outcomes we are driving for GitLab

If you are a team building or seeking to build an AI solution, you are our customer.
Additionally, we provide dashboards, insights, and guidance to empower you to
confidently communicate with YOUR customers using data throughout the process.
Some examples might include:
- How did you select the appropriate model for a use case?
- How did you systematically evaluate your AI solution AT SCALE as a proxy to
production?
- What measures were taken for various prompt engineering techniques?
- Could you explain some of the benchmark datasets you used for evaluation?
- Do we have insight into whether RAG or the AI Agent is truly effective, and how?

And a lot more!

Our current customers include:

1. AI Powered: Duo-Chat team
2. Create: Code Creation team
3. Govern:Threat Insights `Vulnerability explanation` team

##  Top FY25 Priorities

Data Driven evaluated AI Solutions with every code change.

We encompass two categories on AI Evaluation and AI Research. Our goal is to empower each team building AI features to confidently deliver meaningful and relevant features for GitLab customers. As a long-term initiative, we aim to expand our Centralized Evaluation Framework to assess various models, AI features, and components based on quality, cost, and latency. The primary decision factors for AI content quality are:

- Is it honest? (consistent with facts)
- Is it harmless? (not include content that might offend and harm)
- Is it helpful? (accomplishing the end goal of the user)

We also aim for AI Engineers to leverage the Centralized Framework for experimenting and expanding from prompt engineering, RAG, Agent, to model tuning.
This can be achieved through the Framework's API for the Prompt Library, recognizing that every code change significantly impacts the input and output of LLMs.

Further there are novel research topics and we would love GitLab presented in the AI Research community by publishing our approaches on Evaluation!!

## Metrics we love

#### Similarity Score

This metric evaluates the degree of similarity between an answer generated by a point solution and those produced by other LLMs,
such as Claude-2, Text-Bison, and GPT-4, response to the same question or to ground truth.

#### LLM Judge

This metric involves soliciting evaluations from LLM Judges to assess the quality of answers provided given a specific question and context.
Judges are tasked with assigning scores based on three key aspects: correctness, comprehensiveness, and readability.
To enhance the credibility of these scores, multiple LLMs can participate as judges. For instance, if three judges unanimously agree that an answer is subpar, we can confidently conclude its quality.

#### Collective Judge

This metric operates similarly to the "LLM Judge" metric but consolidates all answers generated by each answering model into a single prompt.
Judges are then tasked with comparing these consolidated responses and assigning scores accordingly.

## Short term priorities

Our OKRs can be viewed [here](https://gitlab.com/gitlab-com/gitlab-OKRs/-/issues/?sort=closed_at_desc&state=opened&label_name%5B%5D=group%3A%3Aai%20model%20validation&type%5B%5D=objective&first_page_size=100) (GitLab internal)


### Issues

Our team works across GitLab project including:

- [Prompt library ](https://gitlab.com/gitlab-org/modelops/ai-model-validation-and-research/ai-evaluation/prompt-library/-/issues/?sort=created_date&state=opened&label_name%5B%5D=group%3A%3Aai%20model%20validation&first_page_size=20)
- [Announcements - Weekly updates](https://gitlab.com/gitlab-org/modelops/ai-model-validation-and-research/annoucements/-/issues/?label_name%5B%5D=weekly%20update)
- [GitLab project](https://gitlab.com/groups/gitlab-org/-/boards/7476573?label_name[]=group%3A%3Aai%20model%20validation) issue board


**Required labels**

- Group: `~group::ai model validation`

<!--

## Team Meetings

Our group holds synchronous meetings to gain additional clarity and alignment on our async discussions. We aspire to record all of our meetings as our team members are spread across several time zones and often cannot attend at the scheduled time.

We have a weekly team meeting at 11pm Pacific on Wednesdays (as many team members are in APAC).

Meetings will be in the [`Model Validation Group` playlist in GitLab Unfiltered](https://www.youtube.com/playlist?list=PL05JrBw4t0Kp5fbMFUOKl-uroknEjofJ3)

### Technical Architecture

The team primary codes in python and as part of Ml workflows we build pipelines starting from dataops to mlops. Most of our models our trained using GPU enabled runners and the framework would depend on the use-case. As part of the ML architecture here are the following tools we also use

- Google Data Flow
- Pub/Sub
- Google PostGres

We are always exploring new tools and frameworks to optimize the ML workflow . Currently we here are things we are exploring

- Clickhouse Datastore to store Model data for AI Assisted
- Pytorch as a framework
- Opensource tools for Model Monitoring and Observability
- [Code Suggestion Architecture ] (https://docs.gitlab.com/ee/architecture/blueprints/ai_gateway/#example-feature-code-suggestions)

### Demos

We periodically showcase demos, and if there is any specific demo on Machine learning that would be beneficial, we would love to hear from you. We have monthly demo days when the team presents recent work done in ML, answers questions relevant to ML, and makes tutorials.  Here is a list

- [CI/CD for Machine-learning](https://www.youtube.com/watch?v=7mUgGFgab4E)
- [Suggested Reviewer MLOPS pipeline using GitLab]
- [Prompt Template for LLM's] (https://youtu.be/jR8Czar0cU8)

{{< tableau height="600px" toolbar="hidden" src="https://us-west-2b.online.tableau.com/t/gitlabpublic/views/TopEngineeringMetrics/TopEngineeringMetricsDashboard" >}}
  {{< tableau/filters "GROUP_LABEL"="ai model validation" >}}
{{< /tableau >}}

{{< tableau height="600px" src="https://us-west-2b.online.tableau.com/t/gitlabpublic/views/MergeRequestMetrics/OverallMRsbyType_1" >}}
  {{< tableau/filters "GROUP_LABEL"="ai model validation" >}}
{{< /tableau >}}

{{< tableau height="600px" src="https://us-west-2b.online.tableau.com/t/gitlabpublic/views/Flakytestissues/FlakyTestIssues" >}}
  {{< tableau/filters "GROUP_NAME"="ai model validation" >}}
{{< /tableau >}}

{{< tableau height="600px" src="https://us-west-2b.online.tableau.com/t/gitlabpublic/views/SlowRSpecTestsIssues/SlowRSpecTestsIssuesDashboard" >}}
  {{< tableau/filters "GROUP_LABEL"="ai model validation" >}}
{{< /tableau >}}

-->
